{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD\n",
    "from torchvision import models, transforms\n",
    "import PIL\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "import scipy.ndimage as ndimage\n",
    "%matplotlib inline\n",
    "import scipy.ndimage as nd\n",
    "import PIL.Image\n",
    "from IPython.display import clear_output, Image, display\n",
    "from io import BytesIO\n",
    "\n",
    "def showarray(a, fmt='jpeg'):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    f = BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))\n",
    "    \n",
    "def showtensor(a):\n",
    "    mean = np.array([0.485, 0.456, 0.406]).reshape([1, 1, 3])\n",
    "    std = np.array([0.229, 0.224, 0.225]).reshape([1, 1, 3])\n",
    "    inp = a[0, :, :, :]\n",
    "    inp = inp.transpose(1, 2, 0)\n",
    "    inp = std * inp + mean\n",
    "    inp *= 255\n",
    "    showarray(inp)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "def plot_images(im, titles=None):\n",
    "    plt.figure(figsize=(30, 20))\n",
    "    \n",
    "    for i in range(len(im)):\n",
    "        plt.subplot(10 / 5 + 1, 5, i + 1)\n",
    "        plt.axis('off')\n",
    "        if titles is not None:\n",
    "            plt.title(titles[i])\n",
    "        plt.imshow(im[i])\n",
    "        \n",
    "    plt.pause(0.001)\n",
    "    \n",
    "normalise = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "normalise_resize = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def init_image(size=(400, 400, 3)):\n",
    "    img = PIL.Image.fromarray(np.uint8(np.full(size, 150)))\n",
    "    img = PIL.Image.fromarray(np.uint8(np.random.uniform(150, 180, size)))\n",
    "    img_tensor = normalise(img).unsqueeze(0)\n",
    "    img_np = img_tensor.numpy()\n",
    "    return img, img_tensor, img_np\n",
    "\n",
    "def load_image(path, resize=False, size=None):\n",
    "    img = PIL.Image.open(path)\n",
    "    \n",
    "    if size is not None:\n",
    "        img.thumbnail(size, PIL.Image.ANTIALIAS)\n",
    "        \n",
    "    if resize:\n",
    "        img_tensor = normalise_resize(img).unsqueeze(0)\n",
    "    else:\n",
    "        img_tensor = normalise(img).unsqueeze(0)\n",
    "    img_np = img_tensor.numpy()\n",
    "    return img, img_tensor, img_np\n",
    "\n",
    "def tensor_to_img(t):\n",
    "    a = t.numpy()\n",
    "    mean = np.array([0.485, 0.456, 0.406]).reshape([1, 1, 3])\n",
    "    std = np.array([0.229, 0.224, 0.225]).reshape([1, 1, 3])\n",
    "    inp = a[0, :, :, :]\n",
    "    inp = inp.transpose(1, 2, 0)\n",
    "    inp = std * inp + mean\n",
    "    inp *= 255\n",
    "    inp = np.uint8(np.clip(inp, 0, 255))\n",
    "    return PIL.Image.fromarray(inp)\n",
    "\n",
    "def image_to_variable(image, requires_grad=False, cuda=False):\n",
    "    if cuda:\n",
    "        image = Variable(image.cuda(), requires_grad=requires_grad)\n",
    "    else:\n",
    "        image = Variable(image, requires_grad=requires_grad)\n",
    "    return image\n",
    "\n",
    "def octaver_fn(model, base_img, mask_img, step_fn, octave_n=6, octave_scale=1.4, iter_n=10, **step_args):\n",
    "    octaves = [base_img]\n",
    "    bases = [base_img]\n",
    "    masks = [mask_img]\n",
    "    \n",
    "    for i in range(octave_n - 1):\n",
    "        octaves.append(nd.zoom(octaves[-1], (1, 1, 1.0 / octave_scale, 1.0 / octave_scale), order=1))\n",
    "        bases.append(nd.zoom(bases[-1], (1, 1, 1.0 / octave_scale, 1.0 / octave_scale), order=1))\n",
    "    \n",
    "        if mask_img is not None:\n",
    "            masks.append(nd.zoom(masks[-1], (1, 1, 1.0 / octave_scale, 1.0 / octave_scale), order=1))\n",
    "        else:\n",
    "            masks = [None] * octave_n\n",
    "\n",
    "    detail = np.zeros_like(octaves[-1])\n",
    "    for octave, (octave_base, base_base, mask_base) in enumerate(zip(octaves[::-1], bases[::-1], masks[::-1])):\n",
    "        h, w = octave_base.shape[-2:]\n",
    "        \n",
    "        if octave > 0:\n",
    "            h1, w1 = detail.shape[-2:]\n",
    "            detail = nd.zoom(detail, (1, 1, 1.0 * h / h1, 1.0 * w / w1), order=1)\n",
    "        \n",
    "        src = octave_base + detail\n",
    "        \n",
    "        for i in range(iter_n):\n",
    "            src = step_fn(model, src, base_base, mask_base, **step_args)\n",
    "\n",
    "        detail = src.numpy() - octave_base\n",
    "\n",
    "    return src\n",
    "\n",
    "def filter_step(model, img, layer_index, filter_index, step_size=5, display=True, use_L2=False):\n",
    "    global use_gpu\n",
    "    \n",
    "    mean = np.array([0.485, 0.456, 0.406]).reshape([3, 1, 1])\n",
    "    std = np.array([0.229, 0.224, 0.225]).reshape([3, 1, 1])\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    img_var = image_to_variable(torch.Tensor(img), requires_grad=True, cuda=use_gpu)\n",
    "    optimizer = SGD([img_var], lr=step_size, weight_decay=1e-4)\n",
    "    \n",
    "    x = img_var\n",
    "    for index, layer in enumerate(model.features):\n",
    "        x = layer(x)\n",
    "        if index == layer_index:\n",
    "            break\n",
    "\n",
    "    output = x[0, filter_index]\n",
    "    loss = output.norm() #torch.mean(output)\n",
    "    loss.backward()\n",
    "    \n",
    "    if use_L2:\n",
    "        #L2 normalization on gradients\n",
    "        mean_square = torch.Tensor([torch.mean(img_var.grad.data ** 2) + 1e-5])\n",
    "        if use_gpu:\n",
    "            mean_square = mean_square.cuda()\n",
    "        img_var.grad.data /= torch.sqrt(mean_square)\n",
    "        img_var.data.add_(img_var.grad.data * step_size)\n",
    "    else:\n",
    "        optimizer.step()\n",
    "    \n",
    "    result = img_var.data.cpu().numpy()\n",
    "    result[0, :, :, :] = np.clip(result[0, :, :, :], -mean / std, (1 - mean) / std)\n",
    "    \n",
    "    if display:\n",
    "        showtensor(result)\n",
    "    \n",
    "    return torch.Tensor(result)\n",
    "\n",
    "def visualize_filter(model, base_img, layer_index, filter_index, \n",
    "                     octave_n=6, octave_scale=1.4, iter_n=10, \n",
    "                     step_size=5, display=True, use_L2=False):\n",
    "    \n",
    "    return octaver_fn(\n",
    "                model, base_img, step_fn=filter_step, \n",
    "                octave_n=octave_n, octave_scale=octave_scale, \n",
    "                iter_n=iter_n, layer_index=layer_index, \n",
    "                filter_index=filter_index, step_size=step_size, \n",
    "                display=display, use_L2=use_L2\n",
    "            )\n",
    "\n",
    "def show_layer(layer_num, filter_start=10, filter_end=20, step_size=7, use_L2=False):\n",
    "    filters = []\n",
    "    titles = []\n",
    "    \n",
    "    _, _, img_np = init_image(size=(600, 600, 3))\n",
    "    for i in range(filter_start, filter_end):\n",
    "        title = \"Layer {} Filter {}\".format(layer_num , i)\n",
    "        print(title)\n",
    "        filter = visualize_filter(model, img_np, layer_num, filter_index=i, octave_n=2, iter_n=20, step_size=step_size, display=True, use_L2=use_L2)\n",
    "        filter_img = tensor_to_img(filter)\n",
    "        filter_img.save(title + \".jpg\")\n",
    "        filters.append(tensor_to_img(filter))\n",
    "        titles.append(title)\n",
    "        \n",
    "    \n",
    "    plot_images(filters, titles)\n",
    "    return filters, titles\n",
    "\n",
    "def objective(dst, guide_features):\n",
    "    if guide_features is None:\n",
    "        return dst.data\n",
    "    else:\n",
    "        x = dst.data[0].cpu().numpy()\n",
    "        y = guide_features.data[0].cpu().numpy()\n",
    "        ch, w, h = x.shape\n",
    "        x = x.reshape(ch, -1)\n",
    "        y = y.reshape(ch, -1)\n",
    "        A = x.T.dot(y)\n",
    "        diff = y[:, A.argmax(1)]\n",
    "        if use_gpu:\n",
    "            diff = torch.Tensor(np.array([diff.reshape(ch, w, h)])).cuda()\n",
    "        else: \n",
    "            diff = torch.Tensor(np.array([diff.reshape(ch, w, h)])) \n",
    "        return diff\n",
    "\n",
    "def make_step(model, img, base_img, mask_img, objective=objective, control=None, step_size=1.5, end=28, jitter=32, reg_coeff=1):\n",
    "    global use_gpu\n",
    "    \n",
    "    mean = np.array([0.485, 0.456, 0.406]).reshape([3, 1, 1])\n",
    "    std = np.array([0.229, 0.224, 0.225]).reshape([3, 1, 1])\n",
    "    \n",
    "    ox, oy = np.random.randint(-jitter, jitter+1, 2)\n",
    "    \n",
    "    img = np.roll(np.roll(img, ox, -1), oy, -2)\n",
    "    img_var = image_to_variable(torch.Tensor(img) , requires_grad=True, cuda=use_gpu)\n",
    "    base_var = image_to_variable(torch.Tensor(base_img) , requires_grad=False, cuda=use_gpu)\n",
    "    \n",
    "    if mask_img is not None:\n",
    "        mask_var = image_to_variable(torch.Tensor(mask_img) , requires_grad=False, cuda=use_gpu)\n",
    "    \n",
    "    model.zero_grad()\n",
    "      \n",
    "    x = img_var\n",
    "    \n",
    "    if mask_img is not None:\n",
    "        regularization_loss = torch.nn.L1Loss()\n",
    "        regularization_term = - (torch.abs(x - base_var) * mask_var).sum() * reg_coeff\n",
    "        regularization_term.backward()\n",
    "        \n",
    "    for index, layer in enumerate(model.features.children()):\n",
    "        x = layer(x)\n",
    "        if index == end:\n",
    "            break\n",
    "    \n",
    "    delta = objective(x, control)\n",
    "        \n",
    "    x.backward(delta)\n",
    "    \n",
    "    #L2 Regularization on gradients\n",
    "    mean_square = torch.Tensor([torch.mean(img_var.grad.data ** 2)])\n",
    "    if use_gpu:\n",
    "        mean_square = mean_square.cuda()\n",
    "    img_var.grad.data /= torch.sqrt(mean_square)\n",
    "    img_var.data.add_(img_var.grad.data * step_size)\n",
    "    \n",
    "    result = img_var.data.cpu().numpy()\n",
    "    result = np.roll(np.roll(result, -ox, -1), -oy, -2)\n",
    "    result[0, :, :, :] = np.clip(result[0, :, :, :], -mean / std, (1 - mean) / std)\n",
    "    showtensor(result)\n",
    "    \n",
    "    return torch.Tensor(result)\n",
    "                                                             \n",
    "def deepdream(model, base_img, mask_img=None, octave_n=6, octave_scale=1.4, \n",
    "              iter_n=10, end=28, control=None, objective=objective, \n",
    "              step_size=1.5, jitter=32, reg_coeff=1):\n",
    "    \n",
    "    return octaver_fn(\n",
    "              model, base_img, mask_img=mask_img, step_fn=make_step, \n",
    "              octave_n=octave_n, octave_scale=octave_scale, \n",
    "              iter_n=iter_n, end=end, control=control,\n",
    "              objective=objective, step_size=step_size, jitter=jitter\n",
    "           )\n",
    "\n",
    "def extract_mask(path, size=None, invert=False):\n",
    "\n",
    "    img = PIL.Image.open(path)\n",
    "    \n",
    "    if size is not None:\n",
    "        img.thumbnail(size, PIL.Image.ANTIALIAS)\n",
    "        \n",
    "    img_tensor = transforms.ToTensor()(img)\n",
    "    avg_mask = img_tensor.sum(dim=0, keepdim=True) / 3\n",
    "    avg_mask = avg_mask.expand(3, avg_mask.shape[1], avg_mask.shape[2])\n",
    "    avg_mask = avg_mask.view(1, *avg_mask.shape)\n",
    "    \n",
    "    if invert:\n",
    "        avg_mask = torch.ones(avg_mask.shape) - avg_mask\n",
    "    \n",
    "    return avg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teven/deep/lib/python3.7/site-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "inputImage = 'cancun.jpg'\n",
    "guideImage = 'forest.jpeg'\n",
    "maskImage = \"cancun_mask.png\"\n",
    "\n",
    "model = models.densenet121(pretrained=True)\n",
    "\n",
    "use_gpu = False\n",
    "if torch.cuda.is_available():\n",
    "    use_gpu = True\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if use_gpu:\n",
    "    #print(\"Using CUDA\")\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img, input_tensor, input_np = load_image(inputImage)\n",
    "mask_img = extract_mask(maskImage)\n",
    "#print(input_img.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a814765cdf42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepdream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moctave_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjitter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_coeff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_to_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-6fdeef10d049>\u001b[0m in \u001b[0;36mdeepdream\u001b[0;34m(model, base_img, mask_img, octave_n, octave_scale, iter_n, end, control, objective, step_size, jitter, reg_coeff)\u001b[0m\n\u001b[1;32m    266\u001b[0m               \u001b[0moctave_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moctave_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moctave_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moctave_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m               \u001b[0miter_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m               \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjitter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m            )\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-6fdeef10d049>\u001b[0m in \u001b[0;36moctaver_fn\u001b[0;34m(model, base_img, mask_img, step_fn, octave_n, octave_scale, iter_n, **step_args)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mstep_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mdetail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moctave_base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-6fdeef10d049>\u001b[0m in \u001b[0;36mmake_step\u001b[0;34m(model, img, base_img, mask_img, objective, control, step_size, end, jitter, reg_coeff)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep/lib/python3.7/site-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_DenseLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_rate\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1252\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1253\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m     )\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "dream = deepdream(model, input_np, mask_img=mask_img, end=10, step_size=0.15, octave_n=8, iter_n=10, jitter=0, reg_coeff=100)\n",
    "dream = tensor_to_img(dream)\n",
    "dream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide_img, guide_img_tensor, guide_img_np = load_image(guideImage, resize=True)\n",
    "plt.imshow(guide_img)\n",
    "guide_features = image_to_variable(guide_img_tensor, cuda=use_gpu)\n",
    "end=28\n",
    "for index, layer in enumerate(model.features.children()):\n",
    "    guide_features = layer(guide_features)\n",
    "    if index == end:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dream = deepdream(model, input_np, end=90, step_size=0.06, octave_n=6, control=guide_features)\n",
    "dream = tensor_to_img(dream)\n",
    "dream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(maskImage)\n",
    "print(inputImage)\n",
    "mask_image, mask_image_tensor, mask_image_np = load_image(maskImage, size=[1024, 1024])\n",
    "extract_mask(maskImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
